{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b49247da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\cfpc2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cfpc2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.layers import Activation, Flatten, Bidirectional\n",
    "from keras.layers import Conv1D,MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix, accuracy_score, make_scorer, f1_score,precision_score,recall_score, plot_confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c647d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    train_split = 0.7,\n",
    "    random_state = 42,\n",
    "    vocab_size = 10000,\n",
    "    embedding_dim = 16,\n",
    "    max_length = 120,\n",
    "    batch_size=128,\n",
    "    num_epochs=5,\n",
    "    early_stopping_criteria=2,\n",
    "    dropout_p=0.1,\n",
    "    model_storage=\"model_storage/lstm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2838a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"datasets/reviews.csv\")\n",
    "X = dataset[\"review_comment_message\"].copy()\n",
    "y = dataset[\"review_score\"].copy()\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df3681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(y)):\n",
    "    if y[i] == -1:\n",
    "        y[i] = 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5be6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[1 1 0 ... 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "y_dummy = np_utils.to_categorical(y)\n",
    "print(y_dummy)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb37e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_dummy, test_size=0.3, random_state = 199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803ab284",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tok = \"<OOV>\"\n",
    "tokenizer = Tokenizer(num_words = args.vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "def preprocess(training_sentences, testing_sentences, max_length, vocab_size, trunc_type='post', oov_tok = \"<OOV>\"):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        training_sentences\n",
    "        training_labels\n",
    "        testing_sentences\n",
    "        testing_labels\n",
    "    Return\n",
    "        training_sentences\n",
    "        training_labels\n",
    "        testing_sentences\n",
    "        testing_labels \n",
    "    \"\"\"\n",
    "\n",
    "    stopword = stopwords.words(\"portuguese\")\n",
    "    stem = RSLPStemmer()\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    def clear(review):\n",
    "        review = review.lower()\n",
    "        # remove pula de linha \n",
    "        review = re.sub('\\n', ' ', review)        \n",
    "        review = re.sub('\\r', ' ', review)\n",
    "\n",
    "        # remove numero \n",
    "        review = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' #numero ', review)\n",
    "\n",
    "        # remove caracters especiais \n",
    "        review = re.sub(r'R\\$', ' ', review)\n",
    "        review = re.sub(r'\\W', ' ', review)\n",
    "        review = re.sub(r'\\s+', ' ', review)\n",
    "\n",
    "        # remove links \n",
    "        urls = re.findall('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', review)\n",
    "        if len(urls) > 0:\n",
    "            for url in urls:\n",
    "                for link in url:\n",
    "                    review = review.replace(link, '')\n",
    "            review = review.replace(':', '')\n",
    "            review = review.replace('/', '')\n",
    "        return review\n",
    "\n",
    "    training_sentences = training_sentences.apply(lambda review: clear(review))\n",
    "    testing_sentences = testing_sentences.apply(lambda review: clear(review))\n",
    "    training_sentences = training_sentences.apply(lambda words_review: [word for word in words_review if word not in stopword])\n",
    "    testing_sentences = testing_sentences.apply(lambda words_review: [word for word in words_review if word not in stopword])\n",
    "    # training_sentences = training_sentences.apply(lambda review: word_tokenize(review))\n",
    "    # testing_sentences = testing_sentences.apply(lambda review: word_tokenize(review))\n",
    "    training_sentences = training_sentences.apply(lambda words_review: [stem.stem(word) for word in words_review ])\n",
    "    testing_sentences = testing_sentences.apply(lambda words_review: [stem.stem(word) for word in words_review ])\n",
    "    training_sentences = training_sentences.apply(lambda words_review: \" \".join(words_review))\n",
    "    testing_sentences = testing_sentences.apply(lambda words_review: \" \".join(words_review))\n",
    "    training_sentences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "\n",
    "    training_padded = pad_sequences(training_sentences,maxlen=max_length, truncating=trunc_type)\n",
    "    testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
    "\n",
    "    # training_padded = vectorizer.fit_transform(training_sentences)\n",
    "    # testing_padded = vectorizer.fit_transform(testing_sequences)\n",
    "\n",
    "    return training_padded, testing_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fe9848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new, X_test_new = preprocess(X_train, X_test, args.max_length, args.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72f5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input = tf.keras.Input(shape=(args.max_length))\n",
    "    x = tf.keras.layers.Embedding(args.vocab_size, args.embedding_dim, input_length=args.max_length)(input)\n",
    "\n",
    "    x = tf.keras.layers.LSTM(16, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.LSTM(16)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dropout(0.6)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "    return tf.keras.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "685f3998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "224/224 [==============================] - 42s 174ms/step - loss: 1.0582 - accuracy: 0.4973 - val_loss: 1.0116 - val_accuracy: 0.5004\n",
      "Epoch 2/15\n",
      "224/224 [==============================] - 54s 241ms/step - loss: 1.0041 - accuracy: 0.5187 - val_loss: 0.9732 - val_accuracy: 0.5687\n",
      "Epoch 3/15\n",
      "224/224 [==============================] - 54s 243ms/step - loss: 0.9746 - accuracy: 0.5768 - val_loss: 0.9547 - val_accuracy: 0.5747\n",
      "Epoch 4/15\n",
      "224/224 [==============================] - 55s 243ms/step - loss: 0.9512 - accuracy: 0.5868 - val_loss: 0.9313 - val_accuracy: 0.5886\n",
      "Epoch 5/15\n",
      "224/224 [==============================] - 54s 243ms/step - loss: 0.9262 - accuracy: 0.6004 - val_loss: 0.9028 - val_accuracy: 0.6013\n",
      "Epoch 6/15\n",
      "224/224 [==============================] - 55s 244ms/step - loss: 0.9111 - accuracy: 0.6073 - val_loss: 0.8939 - val_accuracy: 0.6031\n",
      "Epoch 7/15\n",
      "224/224 [==============================] - 48s 215ms/step - loss: 0.9059 - accuracy: 0.6095 - val_loss: 0.8960 - val_accuracy: 0.6061\n",
      "Epoch 8/15\n",
      "224/224 [==============================] - 54s 239ms/step - loss: 0.9031 - accuracy: 0.6095 - val_loss: 0.8938 - val_accuracy: 0.6048\n",
      "Epoch 9/15\n",
      "224/224 [==============================] - 54s 243ms/step - loss: 0.9017 - accuracy: 0.6118 - val_loss: 0.8908 - val_accuracy: 0.6063\n",
      "Epoch 10/15\n",
      "224/224 [==============================] - 53s 235ms/step - loss: 0.8977 - accuracy: 0.6123 - val_loss: 0.8877 - val_accuracy: 0.6074\n",
      "Epoch 11/15\n",
      "224/224 [==============================] - 53s 236ms/step - loss: 0.8972 - accuracy: 0.6116 - val_loss: 0.8870 - val_accuracy: 0.6069\n",
      "Epoch 12/15\n",
      "224/224 [==============================] - 54s 240ms/step - loss: 0.8945 - accuracy: 0.6128 - val_loss: 0.8844 - val_accuracy: 0.6073\n",
      "Epoch 13/15\n",
      "224/224 [==============================] - 53s 238ms/step - loss: 0.8953 - accuracy: 0.6125 - val_loss: 0.8822 - val_accuracy: 0.6085\n",
      "Epoch 14/15\n",
      "224/224 [==============================] - 53s 237ms/step - loss: 0.8937 - accuracy: 0.6131 - val_loss: 0.8899 - val_accuracy: 0.6087\n",
      "Epoch 15/15\n",
      "224/224 [==============================] - 54s 242ms/step - loss: 0.8914 - accuracy: 0.6143 - val_loss: 0.8810 - val_accuracy: 0.6071\n"
     ]
    }
   ],
   "source": [
    "earlyStoppingCallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=args.early_stopping_criteria)\n",
    "\n",
    "model = create_model()\n",
    "model.compile(\n",
    "  loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "  optimizer= tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0001),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_new,\n",
    "    np.array(y_train), \n",
    "    validation_data=(X_test_new, np.array(y_test)),\n",
    "    epochs=15,\n",
    "    batch_size=args.batch_size,\n",
    "    callbacks= [earlyStoppingCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4f79f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/384 [==============================] - 7s 18ms/step - loss: 0.8810 - accuracy: 0.6071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8809857368469238, 0.6071107983589172]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_new, np.array(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
